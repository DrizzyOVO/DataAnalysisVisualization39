{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DrizzyOVO/DataAnalysisVisualization_39/blob/main/DAV_Exp7_39.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fEIrKItviOz2"
      },
      "source": [
        "**Experiment - 7: Perform the steps involved in Text Analytics in Python & R**"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xuTH5erHlhcW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lN5IRo1diVGk"
      },
      "source": [
        "**Task to be performed :**\n",
        "\n",
        "Explore Top-5 Text Analytics Libraries in Python (w.r.t Features & Applications)\n",
        "\n",
        "Explore Top-5 Text Analytics Libraries in R (w.r.t Features & Applications)\n",
        "\n",
        "Perform the following experiments using Python & R\n",
        "\n",
        "Tokenization (Sentence & Word)\n",
        "\n",
        "Frequency Distribution\n",
        "\n",
        "Remove stopwords & punctuations\n",
        "\n",
        "Lexicon Normalization (Stemming, Lemmatization)\n",
        "\n",
        "Part of Speech tagging\n",
        "\n",
        "Named Entity Recognization\n",
        "\n",
        "Scrape data from a website"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PkaePVMelaC2"
      },
      "source": [
        "**Explore Top-5 Text Analytics Libraries in Python**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jK6mYyzSknjn"
      },
      "source": [
        "1. **NLTK (Natural Language Toolkit):**\n",
        "   - **Features:**\n",
        "     - Comprehensive set of libraries and tools for natural language processing (NLP).\n",
        "     - Tokenization, stemming, tagging, parsing, and semantic reasoning functionalities.\n",
        "     - Supports various corpora and lexical resources.\n",
        "     - Provides interfaces to popular resources like WordNet.\n",
        "\n",
        "   - **Applications:**\n",
        "     - Text classification and sentiment analysis.\n",
        "     - Named entity recognition.\n",
        "     - Part-of-speech tagging.\n",
        "     - Concordance and collocation analysis.\n",
        "\n",
        "2. **Scattertext:**\n",
        "   - **Features:**\n",
        "     - Specifically designed for visualizing linguistic variation between document categories.\n",
        "     - Produces interactive scatter plots that highlight terms differentiating categories.\n",
        "     - Supports customization and interactive exploration of visualizations.\n",
        "     - Handles linguistic and stylistic differences well.\n",
        "\n",
        "   - **Applications:**\n",
        "     - Comparative analysis of document categories.\n",
        "     - Identifying distinctive terms in different contexts.\n",
        "     - Visual exploration of language patterns.\n",
        "\n",
        "3. **SpaCy:**\n",
        "   - **Features:**\n",
        "     - Fast and efficient NLP library.\n",
        "     - Tokenization, part-of-speech tagging, named entity recognition, and dependency parsing.\n",
        "     - Pre-trained models for various languages.\n",
        "     - Easy integration with machine learning pipelines.\n",
        "\n",
        "   - **Applications:**\n",
        "     - Named entity recognition and extraction.\n",
        "     - Dependency parsing for understanding relationships between words.\n",
        "     - Text summarization.\n",
        "     - Information extraction.\n",
        "\n",
        "4. **TextBlob:**\n",
        "   - **Features:**\n",
        "     - Simple and intuitive API for common NLP tasks.\n",
        "     - Built on top of NLTK and Pattern libraries.\n",
        "     - Part-of-speech tagging, noun phrase extraction, sentiment analysis, classification, translation, and more.\n",
        "     - Easy to use for beginners.\n",
        "\n",
        "   - **Applications:**\n",
        "     - Sentiment analysis and classification.\n",
        "     - Basic text processing tasks.\n",
        "     - Language translation.\n",
        "     - Parsing and extracting information from text.\n",
        "\n",
        "5. **scikit-learn (sklearn):**\n",
        "   - **Features:**\n",
        "     - General-purpose machine learning library with text processing capabilities.\n",
        "     - Text vectorization techniques (TF-IDF, CountVectorizer).\n",
        "     - Integration with other machine learning algorithms for text classification and clustering.\n",
        "     - Comprehensive documentation and community support.\n",
        "\n",
        "   - **Applications:**\n",
        "     - Text classification (e.g., spam detection).\n",
        "     - Clustering and topic modeling.\n",
        "     - Feature extraction and representation.\n",
        "     - Text regression.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "isLDz5sBleJf"
      },
      "source": [
        "**Explore Top-5 Text Analytics Libraries in R**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4WyfKXClM41"
      },
      "source": [
        "1. **shiny:**\n",
        "   - **Features:**\n",
        "     - Web application framework for R.\n",
        "     - Allows for the creation of interactive and dynamic web-based dashboards and applications.\n",
        "     - Well-suited for building user interfaces and visualizations for text analytics applications.\n",
        "     - Integration with other R libraries for data processing and analysis.\n",
        "\n",
        "   - **Applications:**\n",
        "     - Building interactive dashboards for text analysis results.\n",
        "     - Creating user-friendly interfaces for exploring and visualizing text data.\n",
        "     - Incorporating text analytics into web-based applications.\n",
        "\n",
        "2. **tm (Text Mining Package):**\n",
        "   - **Features:**\n",
        "     - Comprehensive package for text mining in R.\n",
        "     - Supports text preprocessing tasks such as cleaning, stemming, and stopword removal.\n",
        "     - Provides functions for creating document-term matrices (DTM) and term-document matrices (TDM).\n",
        "     - Integration with other R packages for statistical analysis.\n",
        "\n",
        "   - **Applications:**\n",
        "     - Document clustering and classification.\n",
        "     - Term frequency analysis.\n",
        "     - Text preprocessing and transformation.\n",
        "     - Integration with machine learning algorithms for text analysis.\n",
        "\n",
        "3. **quanteda:**\n",
        "   - **Features:**\n",
        "     - Modern and flexible package for quantitative text analysis.\n",
        "     - Supports corpus management, document-feature matrices, and various text analysis operations.\n",
        "     - Designed for efficiency and scalability in handling large text datasets.\n",
        "     - Integration with other R packages for statistical analysis and visualization.\n",
        "\n",
        "   - **Applications:**\n",
        "     - Document-feature matrix creation for text analysis.\n",
        "     - Text preprocessing, including tokenization and stemming.\n",
        "     - Sentiment analysis and text classification.\n",
        "     - Topic modeling and exploratory data analysis.\n",
        "\n",
        "4. **quanteda.textstats:**\n",
        "   - **Features:**\n",
        "     - An extension of the quanteda package, specifically focusing on text statistics.\n",
        "     - Provides functions for calculating various text statistics, such as word frequencies, lexical diversity, and readability measures.\n",
        "     - Useful for gaining insights into the linguistic characteristics of a text corpus.\n",
        "     - Complements quanteda's core functionalities for text analysis.\n",
        "\n",
        "   - **Applications:**\n",
        "     - Analyzing word frequencies and patterns in a corpus.\n",
        "     - Assessing the complexity and readability of text.\n",
        "     - Extracting key statistical information about a text dataset.\n",
        "\n",
        "5. **tm.plugin.sentiment:**\n",
        "   - **Features:**\n",
        "     - A plugin for the tm package that focuses on sentiment analysis.\n",
        "     - Enables sentiment analysis on text data by incorporating pre-trained sentiment lexicons.\n",
        "     - Supports the calculation of sentiment scores for individual documents or terms.\n",
        "     - Useful for understanding the emotional tone or sentiment expressed in a text corpus.\n",
        "\n",
        "   - **Applications:**\n",
        "     - Sentiment analysis in text mining projects.\n",
        "     - Assessing the sentiment polarity (positive, negative, neutral) of documents.\n",
        "     - Incorporating sentiment analysis into larger text analytics workflows.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-yftOesVh86N",
        "outputId": "35c856b7-fc86-4728-92ab-58eacf7d0cde"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.5)\n"
          ]
        }
      ],
      "source": [
        "pip install nltk beautifulsoup4\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HEAIe46Mowy-"
      },
      "source": [
        "1: Tokenization (Sentence & Word)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nkJHrqJ1mIQw",
        "outputId": "8f884e94-f05e-4a16-8078-49511b9bbc02"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qHzF1-nllxax",
        "outputId": "7c64cbcb-f362-4bf6-f242-f880dd86628e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sentence Tokenization:\n",
            "['NLTK is a powerful library for natural language processing']\n",
            "\n",
            "Word Tokenization:\n",
            "['NLTK', 'is', 'a', 'powerful', 'library', 'for', 'natural', 'language', 'processing']\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "# Sample text\n",
        "text = \"NLTK is a powerful library for natural language processing\"\n",
        "\n",
        "# Sentence Tokenization\n",
        "sentences = sent_tokenize(text)\n",
        "print(\"Sentence Tokenization:\")\n",
        "print(sentences)\n",
        "\n",
        "# Word Tokenization\n",
        "words = word_tokenize(text)\n",
        "print(\"\\nWord Tokenization:\")\n",
        "print(words)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nEKB28WroqA_"
      },
      "source": [
        " 2: Frequency Distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wHFs-VQSl-fZ",
        "outputId": "1679ec8c-93d1-4c25-90bc-2af302ff4f22"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Frequency Distribution:\n",
            "<FreqDist with 4 samples and 7 outcomes>\n"
          ]
        }
      ],
      "source": [
        "from nltk import FreqDist\n",
        "\n",
        "# Sample words\n",
        "word_list = [\"apple\", \"banana\", \"apple\", \"orange\", \"banana\", \"apple\", \"grape\"]\n",
        "\n",
        "# Calculate frequency distribution\n",
        "freq_dist = FreqDist(word_list)\n",
        "print(\"Frequency Distribution:\")\n",
        "print(freq_dist)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oArWDtwKoeOX"
      },
      "source": [
        "3: Remove Stopwords & Punctuations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fVFmfRqLmYjW",
        "outputId": "0d906219-1690-4f2d-d23d-13bd00bde889"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "   import nltk\n",
        "   nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iWrS6sGpmRcA",
        "outputId": "8eca759e-96b3-4811-d801-97be41ab1c6d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Text after removing stopwords and punctuations:\n",
            "['sample', 'sentence', 'stopwords', 'punctuations']\n"
          ]
        }
      ],
      "source": [
        "from nltk.corpus import stopwords\n",
        "from string import punctuation\n",
        "\n",
        "# Sample text\n",
        "text = \"This is a sample sentence, with some stopwords and punctuations.\"\n",
        "\n",
        "# Remove stopwords and punctuations\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "filtered_text = [word.lower() for word in word_tokenize(text) if word.isalnum() and word.lower() not in stop_words]\n",
        "\n",
        "print(\"Text after removing stopwords and punctuations:\")\n",
        "print(filtered_text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36ZeBl5pobHf"
      },
      "source": [
        "4: Lexicon Normalization (Stemming, Lemmatization)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MAvjnPcqnBNk",
        "outputId": "34f1087d-f410-4ed1-bb18-e777dbaaa94c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vMLF46WqmUOP",
        "outputId": "1efa7eee-4eeb-40f2-efdf-803e38cc1c46"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Stemmed Words:\n",
            "['run', 'better', 'cat', 'dog']\n",
            "\n",
            "Lemmatized Words:\n",
            "['running', 'better', 'cat', 'dog']\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "\n",
        "# Sample words\n",
        "words = [\"running\", \"better\", \"cats\", \"dogs\"]\n",
        "\n",
        "# Stemming\n",
        "porter_stemmer = PorterStemmer()\n",
        "stemmed_words = [porter_stemmer.stem(word) for word in words]\n",
        "print(\"Stemmed Words:\")\n",
        "print(stemmed_words)\n",
        "\n",
        "# Lemmatization\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
        "print(\"\\nLemmatized Words:\")\n",
        "print(lemmatized_words)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PvibJw0foXxO"
      },
      "source": [
        "5: Part of Speech Tagging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W6dU5lP_nMRN",
        "outputId": "70ff43ca-419e-4f41-b980-ee85dacb9f4e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OKUgcO3pme_W",
        "outputId": "1252a1f1-7ce2-423b-d20d-dce94524cf64"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Part of Speech Tagging:\n",
            "[('NLTK', 'NNP'), ('is', 'VBZ'), ('a', 'DT'), ('powerful', 'JJ'), ('library', 'NN'), ('for', 'IN'), ('natural', 'JJ'), ('language', 'NN'), ('processing', 'NN'), ('.', '.')]\n"
          ]
        }
      ],
      "source": [
        "# Sample text\n",
        "text = \"NLTK is a powerful library for natural language processing.\"\n",
        "\n",
        "# Part of Speech Tagging\n",
        "pos_tags = nltk.pos_tag(word_tokenize(text))\n",
        "print(\"Part of Speech Tagging:\")\n",
        "print(pos_tags)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQxASFpKoUXP"
      },
      "source": [
        "6: Named Entity Recognition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WjYt3_ENnncW",
        "outputId": "0f3ed5be-e13b-441b-cdc8-b095b5a8df83"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wt_4_bw-nJyd",
        "outputId": "44a5e602-62ce-47c9-b2cc-8519ce468b8d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Named Entity Recognition:\n",
            "(S\n",
            "  (ORGANIZATION NLTK/NNP)\n",
            "  is/VBZ\n",
            "  a/DT\n",
            "  powerful/JJ\n",
            "  library/NN\n",
            "  for/IN\n",
            "  natural/JJ\n",
            "  language/NN\n",
            "  processing/NN\n",
            "  ./.)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "nltk.download('words')# Sample text\n",
        "text = \"Barack Obama was the 44th President of the United States.\"\n",
        "\n",
        "# Named Entity Recognition\n",
        "from nltk import ne_chunk\n",
        "named_entities = ne_chunk(pos_tags)\n",
        "print(\"Named Entity Recognition:\")\n",
        "print(named_entities)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6V_bmPnDoR3W"
      },
      "source": [
        "7: Scrape data from a website"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o3-KIe5fnifu",
        "outputId": "6ccc26c4-db18-4d15-d9a7-6f4748ea80b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Text extracted from the website:\n",
            "\n",
            "\n",
            "\n",
            "Example Domain\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Example Domain\n",
            "This domain is for use in illustrative examples in documents. You may use this\n",
            "    domain in literature without prior coordination or asking for permission.\n",
            "More information...\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# URL to scrape\n",
        "url = \"https://example.com\"\n",
        "\n",
        "# Make a request to the URL\n",
        "response = requests.get(url)\n",
        "\n",
        "# Parse HTML content\n",
        "soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "# Extract text content from the webpage\n",
        "webpage_text = soup.get_text()\n",
        "\n",
        "print(\"Text extracted from the website:\")\n",
        "print(webpage_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qZZZRVc2n5l0",
        "outputId": "a2c76735-bbb4-4e1d-8e91-a8a159156924"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Text extracted from the Python documentation homepage:\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "3.12.2 Documentation\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "    Theme\n",
            "    \n",
            "Auto\n",
            "Light\n",
            "Dark\n",
            "\n",
            "\n",
            "Download\n",
            "Download these documents\n",
            "Docs by version\n",
            "\n",
            "Python 3.13 (in development)\n",
            "Python 3.12 (stable)\n",
            "Python 3.11 (stable)\n",
            "Python 3.10 (security-fixes)\n",
            "Python 3.9 (security-fixes)\n",
            "Python 3.8 (security-fixes)\n",
            "Python 3.7 (EOL)\n",
            "Python 3.6 (EOL)\n",
            "Python 3.5 (EOL)\n",
            "Python 3.4 (EOL)\n",
            "Python 3.3 (EOL)\n",
            "Python 3.2 (EOL)\n",
            "Python 3.1 (EOL)\n",
            "Python 3.0 (EOL)\n",
            "Python 2.7 (EOL)\n",
            "Python 2.6 (EOL)\n",
            "All versions\n",
            "\n",
            "Other\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# URL to scrape (Python official documentation homepage)\n",
        "url = \"https://docs.python.org/3/\"\n",
        "\n",
        "# Make a request to the URL\n",
        "response = requests.get(url)\n",
        "\n",
        "# Parse HTML content\n",
        "soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "# Extract text content from the webpage\n",
        "webpage_text = soup.get_text()\n",
        "\n",
        "# Display a portion of the extracted text (for brevity)\n",
        "print(\"Text extracted from the Python documentation homepage:\")\n",
        "print(webpage_text[:500])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RjF-Uhp2vIe1"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dsadL3EqvJ5g"
      },
      "source": [
        "# **R**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9AUVff_VoJ2_",
        "outputId": "c7f58e71-aebb-4735-ae95-be06490ce442"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Installing packages into ‘/usr/local/lib/R/site-library’\n",
            "(as ‘lib’ is unspecified)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "install.packages(c(\"shiny\", \"tm\", \"SnowballC\", \"NLP\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "7huScvf0p4ZX",
        "outputId": "c658bcbe-6dc0-49de-a68f-371504a87a0f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Installing packages into ‘/usr/local/lib/R/site-library’\n",
            "(as ‘lib’ is unspecified)\n",
            "\n",
            "also installing the dependencies ‘openNLPdata’, ‘rJava’\n",
            "\n",
            "\n",
            "Warning message in install.packages(c(\"tokenizers\", \"tm\", \"stringr\", \"SnowballC\", :\n",
            "“installation of package ‘rJava’ had non-zero exit status”\n",
            "Warning message in install.packages(c(\"tokenizers\", \"tm\", \"stringr\", \"SnowballC\", :\n",
            "“installation of package ‘openNLPdata’ had non-zero exit status”\n",
            "Warning message in install.packages(c(\"tokenizers\", \"tm\", \"stringr\", \"SnowballC\", :\n",
            "“installation of package ‘openNLP’ had non-zero exit status”\n",
            "Loading required package: NLP\n",
            "\n"
          ]
        },
        {
          "ename": "ERROR",
          "evalue": "Error in library(openNLP): there is no package called ‘openNLP’\n",
          "output_type": "error",
          "traceback": [
            "Error in library(openNLP): there is no package called ‘openNLP’\nTraceback:\n",
            "1. library(openNLP)"
          ]
        }
      ],
      "source": [
        "# Install necessary packages\n",
        "install.packages(c(\"tokenizers\", \"tm\", \"stringr\", \"SnowballC\", \"openNLP\", \"NLP\", \"rvest\", \"udpipe\"))\n",
        "\n",
        "# Load required libraries\n",
        "library(tokenizers)\n",
        "library(tm)\n",
        "library(stringr)\n",
        "library(SnowballC)\n",
        "library(openNLP)\n",
        "library(NLP)\n",
        "library(udpipe)\n",
        "\n",
        "# Sample text\n",
        "text <- \"This is a sample sentence. Tokenization in R is interesting!\"\n",
        "\n",
        "# Sentence Tokenization\n",
        "sent_tokens <- sent_token_annotate(text)$features\n",
        "\n",
        "# Word Tokenization\n",
        "word_tokens <- word_token_annotate(text)$features\n",
        "\n",
        "# Frequency Distribution\n",
        "word_freq <- table(word_tokens)\n",
        "print(word_freq)\n",
        "\n",
        "# Remove stopwords and punctuations\n",
        "stopwords <- stopwords(\"en\")\n",
        "filtered_tokens <- word_tokens[!(word_tokens %in% stopwords) & !word_tokens %in% strsplit(punctuation(), \"\")[[1]]]\n",
        "\n",
        "# Lexicon Normalization (Stemming, Lemmatization)\n",
        "stemmed_tokens <- wordStem(filtered_tokens)\n",
        "lemmatized_tokens <- lemmatize_words(filtered_tokens)\n",
        "\n",
        "# Part of Speech tagging\n",
        "pos_tags <- pos_tag_annotate(text)$features\n",
        "\n",
        "# Named Entity Recognition (NER)\n",
        "# Note: NER requires a pre-trained model, for example, the spaCy model\n",
        "# You can use udpipe for POS tagging, but for NER, you might want to use spaCy in Python\n",
        "# Alternatively, you can explore the 'cleanNLP' package for NER in R\n",
        "\n",
        "# Scraping data from a website\n",
        "library(rvest)\n",
        "\n",
        "# Example: Scraping titles from a website\n",
        "url <- \"https://example.com\"\n",
        "webpage <- read_html(url)\n",
        "titles <- html_text(html_nodes(webpage, \"h2\"))\n",
        "print(titles)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Tokenization (Sentence & Word)"
      ],
      "metadata": {
        "id": "kI1v-6a05Po-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ljkWenY1rrv3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa5bca1d-ded6-44d1-c7bc-0169a164ea47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1] \"Sentences:\"\n",
            "[1] \"This is a sample sentence\"          \" Tokenization is important for NLP\"\n",
            "[1] \"Words:\"\n",
            " [1] \"This\"         \"is\"           \"a\"            \"sample\"       \"sentence.\"   \n",
            " [6] \"Tokenization\" \"is\"           \"important\"    \"for\"          \"NLP.\"        \n"
          ]
        }
      ],
      "source": [
        "# Tokenization (Sentence & Word)\n",
        "text <- \"This is a sample sentence. Tokenization is important for NLP.\"\n",
        "sentences <- strsplit(text, \"\\\\.\")[[1]]\n",
        "words <- unlist(strsplit(text, \"\\\\s+\"))\n",
        "\n",
        "print(\"Sentences:\")\n",
        "print(sentences)\n",
        "print(\"Words:\")\n",
        "print(words)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "install.packages(\"tokenizers\")\n",
        "library(tokenizers)\n",
        "\n",
        "text <- \"This is a sample sentence. Tokenization is important for NLP.\"\n",
        "sentences <- tokenize_sentences(text)\n",
        "words <- tokenize_words(text)\n",
        "\n",
        "print(\"Sentences:\")\n",
        "print(sentences)\n",
        "print(\"Words:\")\n",
        "print(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v0GpXn1S5O5g",
        "outputId": "2a4bac4c-4579-472a-b5cc-f2b484575318"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Installing package into ‘/usr/local/lib/R/site-library’\n",
            "(as ‘lib’ is unspecified)\n",
            "\n",
            "also installing the dependencies ‘Rcpp’, ‘SnowballC’\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1] \"Sentences:\"\n",
            "[[1]]\n",
            "[1] \"This is a sample sentence.\"         \"Tokenization is important for NLP.\"\n",
            "\n",
            "[1] \"Words:\"\n",
            "[[1]]\n",
            " [1] \"this\"         \"is\"           \"a\"            \"sample\"       \"sentence\"    \n",
            " [6] \"tokenization\" \"is\"           \"important\"    \"for\"          \"nlp\"         \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Frequency Distribution"
      ],
      "metadata": {
        "id": "cQdaln4_5cSN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Frequency Distribution\n",
        "word_freq <- table(words)\n",
        "print(\"Word frequency:\")\n",
        "print(word_freq)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZI6tyG2Q5Uaf",
        "outputId": "aa6e3285-9a59-422e-f6e3-d1271849da52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1] \"Word frequency:\"\n",
            "words\n",
            "           a          for    important           is          nlp       sample \n",
            "           1            1            1            2            1            1 \n",
            "    sentence         this tokenization \n",
            "           1            1            1 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Remove stopwords & punctuations"
      ],
      "metadata": {
        "id": "0Q3U0kap5gHT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove stopwords & punctuations\n",
        "stop_words <- c(\"is\", \"a\", \"for\")  # Example list of stopwords\n",
        "filtered_words <- words[!tolower(words) %in% stop_words & !grepl(\"[[:punct:]]\", words)]\n",
        "print(\"Filtered words:\")\n",
        " print(filtered_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aPdpvYOB5Ztl",
        "outputId": "0333a2db-4fa6-4ee9-aaa8-b13cba2dda22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1] \"Filtered words:\"\n",
            "list()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Lexicon Normalization (Stemming, Lemmatization)"
      ],
      "metadata": {
        "id": "EEBtctgS5iUS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "install.packages(\"SnowballC\")\n",
        "library(SnowballC)\n",
        "\n",
        "# Example data\n",
        "filtered_words <- c(\"running\", \"flies\", \"happily\", \"jumps\")\n",
        "\n",
        "# Stemming using SnowballC\n",
        "stemmed_words <- wordStem(filtered_words)\n",
        "\n",
        "# Print results\n",
        "print(\"Stemmed words:\")\n",
        "print(stemmed_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CEGzSBR15p5e",
        "outputId": "e5e4995b-ddbc-4178-9acc-a67ccae49a25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Installing package into ‘/usr/local/lib/R/site-library’\n",
            "(as ‘lib’ is unspecified)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1] \"Stemmed words:\"\n",
            "[1] \"run\"     \"fli\"     \"happili\" \"jump\"   \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Part of Speech tagging"
      ],
      "metadata": {
        "id": "6rmMbebf5tIe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "install.packages(\"udpipe\", dependencies=TRUE)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QfS-coRC9KgG",
        "outputId": "b7f2d9db-9997-41e9-e6e3-38c32dd0b503"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Installing package into ‘/usr/local/lib/R/site-library’\n",
            "(as ‘lib’ is unspecified)\n",
            "\n",
            "also installing the dependencies ‘modeltools’, ‘topicmodels’\n",
            "\n",
            "\n",
            "Warning message in install.packages(\"udpipe\", dependencies = TRUE):\n",
            "“installation of package ‘topicmodels’ had non-zero exit status”\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install and load the udpipe package\n",
        "install.packages(\"udpipe\")\n",
        "library(udpipe)\n",
        "\n",
        "# Download and load the English model\n",
        "ud_model <- udpipe_download_model(language = \"english\")\n",
        "ud_model <- udpipe_load_model(ud_model$file_model)\n",
        "\n",
        "# Example data\n",
        "words <- c(\"running\", \"flies\", \"happily\", \"jumps\")\n",
        "\n",
        "# Annotate for lemmatization\n",
        "x <- udpipe_annotate(ud_model, x = words, doc_id = 1:length(words))\n",
        "lemmatized_words <- as.data.frame(x)$lemma\n",
        "\n",
        "# Print the result\n",
        "print(\"Lemmatized words:\")\n",
        "print(lemmatized_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8qEf0oXz_OeG",
        "outputId": "04a7e14e-89dd-47fb-a0f2-0f68aa7c3ef0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Installing package into ‘/usr/local/lib/R/site-library’\n",
            "(as ‘lib’ is unspecified)\n",
            "\n",
            "Downloading udpipe model from https://raw.githubusercontent.com/jwijffels/udpipe.models.ud.2.5/master/inst/udpipe-ud-2.5-191206/english-ewt-ud-2.5-191206.udpipe to /content/english-ewt-ud-2.5-191206.udpipe\n",
            "\n",
            " - This model has been trained on version 2.5 of data from https://universaldependencies.org\n",
            "\n",
            " - The model is distributed under the CC-BY-SA-NC license: https://creativecommons.org/licenses/by-nc-sa/4.0\n",
            "\n",
            " - Visit https://github.com/jwijffels/udpipe.models.ud.2.5 for model license details.\n",
            "\n",
            " - For a list of all models and their licenses (most models you can download with this package have either a CC-BY-SA or a CC-BY-SA-NC license) read the documentation at ?udpipe_download_model. For building your own models: visit the documentation by typing vignette('udpipe-train', package = 'udpipe')\n",
            "\n",
            "Downloading finished, model stored at '/content/english-ewt-ud-2.5-191206.udpipe'\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1] \"Lemmatized words:\"\n",
            "[1] \"run\"     \"flie\"    \"happily\" \"jump\"   \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Named Entity Recognization"
      ],
      "metadata": {
        "id": "5cIC3l6-6AqW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "install.packages(\"NLP\")\n",
        "install.packages(\"openNLP\")\n",
        "library(openNLP)\n",
        "library(NLP)\n",
        "\n",
        "ner_tags <- maxent_tagger_chunker(filtered_words, pos_tags)\n",
        "print(\"Named Entities:\")\n",
        "print( ner_tags)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "id": "G7wQFn5M6FuG",
        "outputId": "d303bb25-832f-45c4-f13b-88f7d851fc34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Installing package into ‘/usr/local/lib/R/site-library’\n",
            "(as ‘lib’ is unspecified)\n",
            "\n",
            "Installing package into ‘/usr/local/lib/R/site-library’\n",
            "(as ‘lib’ is unspecified)\n",
            "\n",
            "also installing the dependencies ‘openNLPdata’, ‘rJava’\n",
            "\n",
            "\n",
            "Warning message in install.packages(\"openNLP\"):\n",
            "“installation of package ‘rJava’ had non-zero exit status”\n",
            "Warning message in install.packages(\"openNLP\"):\n",
            "“installation of package ‘openNLPdata’ had non-zero exit status”\n",
            "Warning message in install.packages(\"openNLP\"):\n",
            "“installation of package ‘openNLP’ had non-zero exit status”\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ERROR",
          "evalue": "Error in library(openNLP): there is no package called ‘openNLP’\n",
          "traceback": [
            "Error in library(openNLP): there is no package called ‘openNLP’\nTraceback:\n",
            "1. library(openNLP)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Scrape data from a website"
      ],
      "metadata": {
        "id": "cZRTIHl96Lyt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "install.packages(\"rvest\")\n",
        "library(rvest)\n",
        "\n",
        "url <- \"https://example.com\"\n",
        "page <- read_html(url)\n",
        "text_data <- page %>%\n",
        "  html_text()\n",
        "\n",
        "print(\"Text data from website:\")\n",
        "print(text_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x_dpRKFl6MmV",
        "outputId": "a695e56c-4a47-4972-b456-17458315dba2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Installing package into ‘/usr/local/lib/R/site-library’\n",
            "(as ‘lib’ is unspecified)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1] \"Text data from website:\"\n",
            "[1] \"Example Domain\\n    body {\\n        background-color: #f0f0f2;\\n        margin: 0;\\n        padding: 0;\\n        font-family: -apple-system, system-ui, BlinkMacSystemFont, \\\"Segoe UI\\\", \\\"Open Sans\\\", \\\"Helvetica Neue\\\", Helvetica, Arial, sans-serif;\\n        \\n    }\\n    div {\\n        width: 600px;\\n        margin: 5em auto;\\n        padding: 2em;\\n        background-color: #fdfdff;\\n        border-radius: 0.5em;\\n        box-shadow: 2px 3px 7px 2px rgba(0,0,0,0.02);\\n    }\\n    a:link, a:visited {\\n        color: #38488f;\\n        text-decoration: none;\\n    }\\n    @media (max-width: 700px) {\\n        div {\\n            margin: 0 auto;\\n            width: auto;\\n        }\\n    }\\n    \\n\\n    Example Domain\\n    This domain is for use in illustrative examples in documents. You may use this\\n    domain in literature without prior coordination or asking for permission.\\n    More information...\\n\\n\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Outcome :\n",
        "Identified the Text Analytics Libraries in Python and R.\n",
        "Performed simple experiments with these libraries in Python and R."
      ],
      "metadata": {
        "id": "CwySvT9n6QbH"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "R",
      "name": "ir"
    },
    "language_info": {
      "name": "R"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}